{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e599e7d-9f70-46aa-aa1b-c3e50fa37556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "fourier enhanced block used!\n",
      "modes=64, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "fourier enhanced block used!\n",
      "modes=64, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      " fourier enhanced cross attention used!\n",
      "modes_q=27, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "modes_kv=18, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "enc_modes: 18, dec_modes: 27\n",
      ">>>>>>>start training : train_FEDformer_random_modes64_custom_ftMS_sl36_ll18_pl36_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "not scaling\n",
      "train 662\n",
      "not scaling\n",
      "val 71\n",
      "not scaling\n",
      "test 174\n",
      "running the training now\n",
      "Epoch: 1 cost time: 14.105525732040405\n",
      "Epoch: 1, Steps: 20 | Train Loss: 0.1671475 Vali Loss: 0.0300011 Test Loss: 0.0309815\n",
      "Validation loss decreased (inf --> 0.030001).  Saving model ...\n",
      "Updating learning rate to 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp(settings)  \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m--> 144\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\machine learning iim\\FEDformer\\exp\\exp_main.py:113\u001b[0m, in \u001b[0;36mExp_Main.train\u001b[1;34m(self, setting)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    112\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_x, batch_y, batch_x_mark, batch_y_mark) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    114\u001b[0m     iter_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    115\u001b[0m     model_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Desktop\\machine learning iim\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\machine learning iim\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\machine learning iim\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from exp.exp_main import Exp_Main\n",
    "from data_provider.data_factory import data_provider\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.timefeatures import time_features\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from layers.Embed import DataEmbedding, DataEmbedding_wo_pos\n",
    "from layers.AutoCorrelation import AutoCorrelation, AutoCorrelationLayer\n",
    "from layers.FourierCorrelation import FourierBlock, FourierCrossAttention\n",
    "from layers.MultiWaveletCorrelation import MultiWaveletCross, MultiWaveletTransform\n",
    "from layers.SelfAttention_Family import FullAttention, ProbAttention\n",
    "from layers.Autoformer_EncDec import Encoder, Decoder, EncoderLayer, DecoderLayer, my_Layernorm, series_decomp, series_decomp_multi\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "fix_seed = int((time.time() * 1000) % 2**32)\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    # Basic configuration\n",
    "    is_training = 1  # status, default 1\n",
    "    task_id = 'train'  # task id, default 'test'\n",
    "    model = 'FEDformer'  # model name, options: [FEDformer, Autoformer, Informer, Transformer], default 'FEDformer'\n",
    "\n",
    "    # Supplementary configuration for FEDformer model\n",
    "    version = 'Fourier'  # options: [Fourier, Wavelets], default 'Fourier'\n",
    "    mode_select = 'random'  # mode selection method, options: [random, low], default 'random'\n",
    "    modes = 64  # modes to be selected randomly, default 64\n",
    "    L = 3  # ignore level, default 3\n",
    "    base = 'legendre'  # mwt base, default 'legendre'\n",
    "    cross_activation = 'tanh'  # mwt cross attention activation function, options: [tanh, softmax], default 'tanh'\n",
    "\n",
    "    # Data loader configuration\n",
    "    data = 'custom'  # dataset type, default 'ETTh1' set 'custom' for own data\n",
    "    root_path = ''  # root path of the data file, default './dataset/ETT/'\n",
    "    data_path = 'tr_EURUSD=X_1h_14.csv'  # data file, default 'ETTh1.csv'\n",
    "    features = 'MS'  # forecasting task, options: [M, S, MS], default 'M'\n",
    "    target = 'Close'  # target feature in S or MS task, default 'OT'\n",
    "    freq = 't'  # frequency for time features encoding, options: [s, t, h, d, b, w, m], default 'h'\n",
    "    detail_freq = 't'  # detailed frequency, default help='like freq, but use in predict'\n",
    "    checkpoints = './checkpoints/'  # location of model checkpoints, default './checkpoints/'\n",
    "\n",
    "    # Forecasting task configuration\n",
    "    seq_len = 36  # input sequence length, default 96\n",
    "    label_len = 18  # start token length, default 48\n",
    "    pred_len = 36  # prediction sequence length, default 96\n",
    "\n",
    "    # Model definition\n",
    "    enc_in = 13  # encoder input size, default 7\n",
    "    dec_in = 13  # decoder input size, default 7\n",
    "    \n",
    "    c_out = 1  # output size, default 7\n",
    "    d_model = 512  # dimension of model, default 512 i was getting nan if itrained my model on 51\n",
    "    n_heads = 8  # number of heads, default 8\n",
    "    e_layers = 2  # number of encoder layers, default 2\n",
    "    d_layers = 1  # number of decoder layers, default 1\n",
    "    d_ff = 2048  # dimension of fcn, default 2048\n",
    "    \n",
    "    moving_avg = 6  # window size of moving average, default [24]\n",
    "    \n",
    "    factor = 1  # attention factor, default 1\n",
    "    distil = True  # whether to use distilling in encoder, default True\n",
    "    \n",
    "    dropout = 0.09  # dropout, default 0.05 --\n",
    "    \n",
    "    embed = 'timeF'  # time features encoding, options: [timeF, fixed, learned], default 'timeF' if timeF no time encoding\n",
    "    activation = 'gelu'  # activation, default 'gelu'\n",
    "    output_attention = False  # whether to output attention in encoder, default False\n",
    "    do_predict = False  # whether to predict unseen future data, default False\n",
    "\n",
    "    # Optimization configuration\n",
    "    num_workers = 2  # data loader number of workers, default 10s\n",
    "    # itr = 3               # experiments times, default 3 this is not equal to no of epochs\n",
    "    train_epochs = 10 # train epochs, default 10\n",
    "    batch_size = 32        # batch size of train input data, default 32\n",
    "    patience = 3            # early stopping patience, default 3\n",
    "    learning_rate = 0.0001  # optimizer learning rate, default 0.0001\n",
    "    # des = 'Exp'           # experiment description, default 'test'\n",
    "    loss = 'mse'            # loss funnction, default 'mse'\n",
    "    lradj = 'type3'         # adjust learning rate, default 'type1'\n",
    "    use_amp = False        # use automatic mixed precision training, default False\n",
    "\n",
    "    # GPU configuration\n",
    "    use_gpu = True  # use GPU, default True\n",
    "    gpu = 0  # GPU, default 0\n",
    "    use_multi_gpu = False  # use multiple GPUs, default False\n",
    "    devices = '0,1'  # device ids of multiple GPUs, default '0,1'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "settings = Configs()\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n",
    "\n",
    "setting = '{}_{}_{}_modes{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(\n",
    "    settings.task_id,\n",
    "    settings.model,\n",
    "    settings.mode_select,\n",
    "    settings.modes,\n",
    "    settings.data,\n",
    "    settings.features,\n",
    "    settings.seq_len,\n",
    "    settings.label_len,\n",
    "    settings.pred_len,\n",
    "    settings.d_model,\n",
    "    settings.n_heads,\n",
    "    settings.e_layers,\n",
    "    settings.d_layers,\n",
    "    settings.d_ff,\n",
    "    settings.factor,\n",
    "    settings.embed,\n",
    "    settings.distil,\n",
    "    # settings.des,\n",
    "    0)\n",
    "\n",
    "exp = Exp(settings)  # set experiments\n",
    "print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "model = exp.train(setting)\n",
    "print('training finished')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432651b-5013-4dd1-98dd-62a8625c3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = sum(p.numel() for p in model.parameters())\n",
    "print('total number of parameter in this model is {}'.format(model_parameters))\n",
    "model_size_bytes = model_parameters * 4  # Assuming float32 weights\n",
    "model_size_kb = model_size_bytes / 1024  # Convert bytes to kilobytes\n",
    "print(\"Model size in KB:\", model_size_kb)\n",
    "print(\"Model size in MB:\", model_size_kb//1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681229c-4cf0-41c4-a487-678a2d4df075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data_provider.data_loader import Dataset_Pred\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "timeenc = 0 if settings.embed != 'timeF' else 1\n",
    "\n",
    "\n",
    "data_set = Dataset_Pred(settings.root_path,flag='pred',size=[settings.seq_len, settings.label_len, settings.pred_len],\n",
    "                        features= settings.features, data_path='val_EURUSD=X_1h_14.csv', target=settings.target, scale=False, \n",
    "                        inverse=False, timeenc=1, freq=settings.freq, cols=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = data_set.__getitem__(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_x = torch.tensor(batch_x)\n",
    "batch_x = batch_x.unsqueeze(0)\n",
    "batch_y = torch.tensor(batch_y)\n",
    "batch_y = batch_y.unsqueeze(0)\n",
    "batch_x_mark = torch.tensor(batch_x_mark)\n",
    "batch_x_mark = batch_x_mark.unsqueeze(0)\n",
    "batch_y_mark = torch.tensor(batch_y_mark)\n",
    "batch_y_mark = batch_y_mark.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('shapes:')\n",
    "print('batch_x',batch_x.shape)\n",
    "    # print('batch_x',batch_x.round(7))\n",
    "\n",
    "print('batch_x_mark.shape',batch_x_mark.shape)\n",
    "print('batch_y',batch_y.shape)\n",
    "print('batch_y_mark.shape',batch_y_mark.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_x = batch_x.float().to(device)\n",
    "batch_y = batch_y.float().to(device)\n",
    "batch_x_mark = batch_x_mark.float().to(device)\n",
    "batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "    #     # decoder input\n",
    "    # dec_inp = torch.zeros_like(batch_y[:, -settings.pred_len:, :]).float()\n",
    "    # dec_inp = torch.cat([batch_y[:, :settings.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    # encoder - decoder\n",
    "    # outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "outputs = model.forward(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "\n",
    "\n",
    "    # prep output to be furter detransfrormed and printed as chart\n",
    "outputs = outputs.detach().cpu().numpy().squeeze()\n",
    "outputs = outputs.reshape(-1, 1)\n",
    "\n",
    "batch_y = batch_y.detach().cpu().numpy().squeeze()\n",
    "batch_y = batch_y.reshape(-1, 1)\n",
    "\n",
    "batch_x = batch_x.detach().cpu().numpy().squeeze()\n",
    "batch_x = batch_x.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plotting\n",
    "\n",
    "# plt.plot(outputs , color='g')\n",
    "# plt.plot(batch_x,color='b')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8ac2b-1c52-484b-adb7-653ac96dc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df_val = pd.read_csv('val_EURUSD=X_1h_14.csv') \n",
    "df_te = pd.read_csv('te_EURUSD=X_1h_14.csv')\n",
    "\n",
    "y_true = df_te[['Close']].values\n",
    "x_val = df_val[['Close']].values\n",
    "\n",
    "# print(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed1dc8-3a8b-4722-a64a-fd5835131ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "dates = pd.date_range('20230101', periods=36)  # Dummy date range\n",
    "test_data = x_val.flatten()\n",
    "true_data = y_true.flatten()\n",
    "concatenated_array = np.concatenate((test_data, true_data))\n",
    "# print(concatenated_array)\n",
    "\n",
    "predicted_data = outputs\n",
    "print(predicted_data.shape)\n",
    "print(true_data.shape)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot( concatenated_array, label='Test Data', color='blue')\n",
    "# plt.plot(test_data, label='True Data', color='green')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Comparison of Test, True, and Predicted Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "plt.plot(predicted_data, label='Predicted Data', color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992d480-d75e-4d17-94e6-430e9d205de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5983f-2339-40f8-b61e-c119fa29354b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29955e0-2f32-439f-abdc-f97a000b8bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
